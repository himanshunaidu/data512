{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Politician Articles: Bias\n",
    "\n",
    "This notebook explores bias in data using Wikipedia articles about political figures from different countries. The analysis aims to examine the coverage of politicians on Wikipedia and the quality of articles about politicians across nations. \n",
    "\n",
    "The notebook utilizes data from two sources: \n",
    "- Dataset of Wikipedia articles about politicians using Wikipedia [Category:Politicians_by_nationality](https://en.wikipedia.org/wiki/Category:Politicians_by_nationality).\n",
    "- Dataset of country populations obtained from [World Population Data Sheet](https://www.prb.org/international/indicator/population/table/).\n",
    "\n",
    "Additionally, the machine learning service ORES is used to estimate the quality of each article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article Page Info MediaWiki API\n",
    "\n",
    "The following sub-section contains code to access page info data using the [MediaWiki REST API for the EN Wikipedia](https://www.mediawiki.org/wiki/API:Main_page). This sub-section shows how to request summary 'page info' for a single article page. The API documentation, [API:Info](https://www.mediawiki.org/wiki/API:Info), covers additional details that may be helpful when trying to use or understand this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### License\n",
    "\n",
    "This code example was developed by Dr. David W. McDonald for use in DATA 512, a course in the UW MS Data Science degree program. This code is provided under the [Creative Commons](https://creativecommons.org) [CC-BY license](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "Modifications to this code were made by Himanshu Naidu on October 13, 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# These are standard python modules\n",
    "import json, time, urllib.parse\n",
    "#\n",
    "# The 'requests' module is not a standard Python module. You will need to install this with pip/pip3 if you do not already have it\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example relies on some constants that help make the code a bit more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "# The basic English Wikipedia API endpoint\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "API_HEADER_AGENT = 'User-Agent'\n",
    "\n",
    "# We'll assume that there needs to be some throttling for these requests - we should always be nice to a free data resource\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making automated requests we should include something that is unique to the person making the request\n",
    "# This should include an email - your UW email would be good to put in there\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<hnaidu36@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2024'\n",
    "}\n",
    "\n",
    "# This is just a list of English Wikipedia article titles that we can use for example requests\n",
    "ARTICLE_TITLES = [ 'Bison', 'Northern flicker', 'Red squirrel', 'Chinook salmon', 'Horseshoe bat' ]\n",
    "\n",
    "# This is a string of additional page properties that can be returned see the Info documentation for\n",
    "# what can be included. If you don't want any this can simply be the empty string\n",
    "PAGEINFO_EXTENDED_PROPERTIES = \"talkid|url|watched|watchers\"\n",
    "#PAGEINFO_EXTENDED_PROPERTIES = \"\"\n",
    "\n",
    "# This template lists the basic parameters for making this\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",           # to simplify this should be a single page title at a time\n",
    "    \"prop\": \"info\",\n",
    "    \"inprop\": PAGEINFO_EXTENDED_PROPERTIES\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API request will be made using one procedure. The idea is to make this reusable. The procedure is parameterized, but relies on the constants above for the important parameters. The underlying assumption is that this will be used to request data for a set of article pages. Therefore the parameter most likely to change is the article_title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_pageinfo_per_article(article_title = None, \n",
    "                                 endpoint_url = API_ENWIKIPEDIA_ENDPOINT, \n",
    "                                 request_template = PAGEINFO_PARAMS_TEMPLATE,\n",
    "                                 headers = REQUEST_HEADERS):\n",
    "    \n",
    "    # article title can be as a parameter to the call or in the request_template\n",
    "    if article_title:\n",
    "        request_template['titles'] = article_title\n",
    "\n",
    "    if not request_template['titles']:\n",
    "        raise Exception(\"Must supply an article title to make a pageinfo request.\")\n",
    "\n",
    "    if API_HEADER_AGENT not in headers:\n",
    "        raise Exception(f\"The header data should include a '{API_HEADER_AGENT}' field that contains your UW email address.\")\n",
    "\n",
    "    if 'uwnetid@uw' in headers[API_HEADER_AGENT]:\n",
    "        raise Exception(f\"Use your UW email address in the '{API_HEADER_AGENT}' field.\")\n",
    "\n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or any other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(endpoint_url, headers=headers, params=request_template)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting page info data for: Chinook salmon\n",
      "{\n",
      "    \"batchcomplete\": \"\",\n",
      "    \"query\": {\n",
      "        \"pages\": {\n",
      "            \"1212891\": {\n",
      "                \"pageid\": 1212891,\n",
      "                \"ns\": 0,\n",
      "                \"title\": \"Chinook salmon\",\n",
      "                \"contentmodel\": \"wikitext\",\n",
      "                \"pagelanguage\": \"en\",\n",
      "                \"pagelanguagehtmlcode\": \"en\",\n",
      "                \"pagelanguagedir\": \"ltr\",\n",
      "                \"touched\": \"2024-10-12T10:13:54Z\",\n",
      "                \"lastrevid\": 1234351318,\n",
      "                \"length\": 53787,\n",
      "                \"watchers\": 109,\n",
      "                \"talkid\": 3909817,\n",
      "                \"fullurl\": \"https://en.wikipedia.org/wiki/Chinook_salmon\",\n",
      "                \"editurl\": \"https://en.wikipedia.org/w/index.php?title=Chinook_salmon&action=edit\",\n",
      "                \"canonicalurl\": \"https://en.wikipedia.org/wiki/Chinook_salmon\"\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Getting page info data for: {ARTICLE_TITLES[3]}\")\n",
    "info = request_pageinfo_per_article(ARTICLE_TITLES[3])\n",
    "print(json.dumps(info,indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requesting ORES scores through LiftWing ML Service API\n",
    "\n",
    "This sub-section illustrates how to generate article quality estimates for article revisions using the LiftWing version of ORES. The ORES API documentation can be accessed from the main ORES page. The ORES LiftWing documentation is very thin ... even thinner than the standard ORES documentation. Further, it is clear that some parameters have been renamed (e.g., \"revid\" in the old ORES API is now \"rev_id\" in the LiftWing ORES API)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### License\n",
    "This code example was developed by Dr. David W. McDonald for use in DATA 512, a course in the UW MS Data Science degree program. This code is provided under the [Creative Commons](https://creativecommons.org) [CC-BY license](https://creativecommons.org/licenses/by/4.0/). \n",
    "\n",
    "Modifications to this code were made by Himanshu Naidu on October 13, 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "#    The current LiftWing ORES API endpoint and prediction model\n",
    "#\n",
    "API_ORES_LIFTWING_ENDPOINT = \"https://api.wikimedia.org/service/lw/inference/v1/models/{model_name}:predict\"\n",
    "API_ORES_EN_QUALITY_MODEL = \"enwiki-articlequality\"\n",
    "\n",
    "#\n",
    "#    The throttling rate is a function of the Access token that you are granted when you request the token. The constants\n",
    "#    come from dissecting the token and getting the rate limits from the granted token. An example of that is below.\n",
    "#\n",
    "API_ORES_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_ORES_THROTTLE_WAIT = ((60.0*60.0)/5000.0)-API_ORES_LATENCY_ASSUMED  # The key authorizes 5000 requests per hour\n",
    "\n",
    "#    When making automated requests we should include something that is unique to the person making the request\n",
    "#    This should include an email - your UW email would be good to put in there\n",
    "#    \n",
    "#    Because all LiftWing API requests require some form of authentication, you need to provide your access token\n",
    "#    as part of the header too\n",
    "#\n",
    "REQUEST_HEADER_TEMPLATE = {\n",
    "    'User-Agent': \"<{email_address}>, University of Washington, MSDS DATA 512 - AUTUMN 2024\",\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': \"Bearer {access_token}\"\n",
    "}\n",
    "#\n",
    "#    This is a template for the parameters that we need to supply in the headers of an API request\n",
    "#\n",
    "REQUEST_HEADER_PARAMS_TEMPLATE = {\n",
    "    'email_address' : \"\",         # your email address should go here\n",
    "    'access_token'  : \"\"          # the access token you create will need to go here\n",
    "}\n",
    "\n",
    "#\n",
    "#    A dictionary of English Wikipedia article titles (keys) and sample revision IDs that can be used for this ORES scoring example\n",
    "#\n",
    "ARTICLE_REVISIONS = { 'Bison':1085687913 , 'Northern flicker':1086582504 , 'Red squirrel':1083787665 , 'Chinook salmon':1085406228 , 'Horseshoe bat':1060601936 }\n",
    "\n",
    "#\n",
    "#    This is a template of the data required as a payload when making a scoring request of the ORES model\n",
    "#\n",
    "ORES_REQUEST_DATA_TEMPLATE = {\n",
    "    \"lang\":        \"en\",     # required that its english - we're scoring English Wikipedia revisions\n",
    "    \"rev_id\":      \"\",       # this request requires a revision id\n",
    "    \"features\":    True\n",
    "}\n",
    "\n",
    "#\n",
    "#    These are used later - defined here so they, at least, have empty values\n",
    "#\n",
    "USERNAME = \"\"\n",
    "ACCESS_TOKEN = \"\"\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get your access token\n",
    "\n",
    "You will need a Wikimedia user account to get access to Lift Wing (the ML API service). You can either [create an account or login](https://api.wikimedia.org/w/index.php?title=Special:UserLogin). If you have a Wikipedia user account - you might already have an Wikimedia account. If you are not sure try your Wikipedia username and password to check it. If you do not have a Wikimedia account you will need to create an account that you can use to get an access token.\n",
    "\n",
    "There is [a 'guide' that describes how to get authentication tokens](https://api.wikimedia.org/wiki/Authentication) - but not everything works the way it is described in that documentation. You should review that documentation and then read the rest of this comment.\n",
    "\n",
    "The documentation talks about using a \"dashboard\" for managing authentication tokens. That's a rather generous description for what looks like a simple list of token things. You might have a hard time finding this \"dashboard\". First, on the left hand side of the page, you'll see a column of links. The bottom section is a set of links titled \"Tools\". In that section is a link that says [Special pages](https://api.wikimedia.org/wiki/Special:SpecialPages) which will take you to a list of ... well, special pages. At the very bottom of the \"Special pages\" page is a section titled \"Other special pages\" (scroll all the way to the bottom). The first link in that section is called [API keys](https://api.wikimedia.org/wiki/Special:AppManagement). When you get to the \"API keys\" page you can create a new key.\n",
    "\n",
    "The authentication guide suggests that you should create a server-side app key. This does not seem to work correctly - as yet. It failed on multiple attempts when I attempted to create a server-side app key. BUT, there is an option to create a [Personal API token](https://api.wikimedia.org/wiki/Authentication) that should work for this course and the type of ORES page scoring that you will need to perform.\n",
    "\n",
    "Note, when you create a Personal API token you are granted the three items - a Client ID, a Client secret, and a Access token - you shold save all three of these. When you dismiss the box they are gone. If you lose any one of the tokens you can destroy or deactivate the Personal API token from the dashboard and then create a new one.\n",
    "\n",
    "The value you need to work the code below is the Access token - a very long string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Once you've done the right set up with your Wikimedia account, it should provide you with three different keys, a Client ID,\n",
    "#   a Client secret, and a Access token.\n",
    "#\n",
    "#   In this case I don't want to distribute my keys with the source of the notebook, so I used the dotenv package to load them from\n",
    "#   a file called '.env' in the same directory as this notebook. The file should look like this:\n",
    "#\n",
    "#   WIKIMEDIA_USERNAME=\"<your_wikimedia_username>\"\n",
    "#   WIKIMEDIA_CLIENT_ID=\"<your_wikimedia_client_id>\"\n",
    "#   WIKIMEDIA_CLIENT_SECRET=\"<your_wikimedia_client_secret>\"\n",
    "#   WIKIMEDIA_ACCESS_TOKEN=\"<your_wikimedia_provided_access_token_its_a_really_long_string>\"\n",
    "#\n",
    "#   The repository should have a file called '.env.example' that you can copy to '.env' and fill in the values.\n",
    "\n",
    "# USERNAME = \"<your_wikimedia_username>\"\n",
    "# ACCESS_TOKEN = \"<your_wikimedia_provided_access_token_its_a_really_long_string>\"\n",
    "\n",
    "# Note: The above properties will be assigned in the \"Getting Article Quality Predictions\" section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to make the ORES API request\n",
    "\n",
    "The API request will be made using a function to encapsulate call and make access reusable in other notebooks. The procedure is parameterized, relying on the constants above for some important default parameters. The primary assumption is that this function will be used to request data for a set of article revisions. The main parameter is 'article_revid'. One should be able to simply pass in a new article revision id on each call and get back a python dictionary as the result. A valid result will be a dictionary that contains the probabilities that the specific revision is one of six different article quality levels. Generally, quality level with the highest probability score is considered the quality level for the article. This can be tricky when you have two (or more) highly probable quality levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_ores_score_per_article(article_revid = None, email_address=None, access_token=None,\n",
    "                                   endpoint_url = API_ORES_LIFTWING_ENDPOINT, \n",
    "                                   model_name = API_ORES_EN_QUALITY_MODEL, \n",
    "                                   request_data = ORES_REQUEST_DATA_TEMPLATE, \n",
    "                                   header_format = REQUEST_HEADER_TEMPLATE, \n",
    "                                   header_params = REQUEST_HEADER_PARAMS_TEMPLATE):\n",
    "    \n",
    "    #    Make sure we have an article revision id, email and token\n",
    "    #    This approach prioritizes the parameters passed in when making the call\n",
    "    if article_revid:\n",
    "        request_data['rev_id'] = article_revid\n",
    "    if email_address:\n",
    "        header_params['email_address'] = email_address\n",
    "    if access_token:\n",
    "        header_params['access_token'] = access_token\n",
    "    \n",
    "    #   Making a request requires a revision id - an email address - and the access token\n",
    "    if not request_data['rev_id']:\n",
    "        raise Exception(\"Must provide an article revision id (rev_id) to score articles\")\n",
    "    if not header_params['email_address']:\n",
    "        raise Exception(\"Must provide an 'email_address' value\")\n",
    "    if not header_params['access_token']:\n",
    "        raise Exception(\"Must provide an 'access_token' value\")\n",
    "    \n",
    "    # Create the request URL with the specified model parameter - default is a article quality score request\n",
    "    request_url = endpoint_url.format(model_name=model_name)\n",
    "    \n",
    "    # Create a compliant request header from the template and the supplied parameters\n",
    "    headers = dict()\n",
    "    for key in header_format.keys():\n",
    "        headers[str(key)] = header_format[key].format(**header_params)\n",
    "    \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free data\n",
    "        # source like ORES - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        #response = requests.get(request_url, headers=headers)\n",
    "        response = requests.post(request_url, headers=headers, data=json.dumps(request_data))\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Article and Population Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    IMPORTS\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "# Dev environment flag to control the number of requests made to the API for testing\n",
    "DEV_ENVIRONMENT = False\n",
    "\n",
    "# The cleaned CSV file path that contains the list of politicians by country\n",
    "POLITICIANS_BY_COUNTRY_CSV_PATH = \"politicians_by_country_AUG.2024.csv\"\n",
    "# The cleaned CSV file path that contains the list of population by country\n",
    "POPULATION_BY_COUNTRY_CSV_PATH = \"population_by_country_AUG.2024.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated in the [MediaWiki REST API Help](https://www.mediawiki.org/w/api.php?action=help&modules=query): \n",
    "\n",
    "For titles \"Maximum number of values is 50 (500 for clients that are allowed higher limits).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be on the safe side, we'll limit the number of titles to 40\n",
    "TITLE_LIMIT = 40\n",
    "# The separator for the titles in the query string\n",
    "TITLE_SEPARATOR = \"|\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the error list file, that includes the articles whose page info could not be retrieved\n",
    "ERROR_LIST_PATH = \"data/errors.txt\" \n",
    "# The path to the output file that contains the page info data\n",
    "WP_POLITICIANS_WITH_PAGE_INFO_PATH = \"data/wp_politicians_with_page_info.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a way to get the page information for multiple pages at the same time, by separating the page titles with the vertical bar \"|\" character. However, this approach has limits. You should probably check the API documentation if you want to do multiple pages in a single request - and limit the number of pages in one request reasonably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_page_info_multiple_articles(titles = None, title_sep = '|', error_list = None):\n",
    "    '''\n",
    "    This function takes a list of article titles and returns the page information for all the articles in a list.\n",
    "    This function also adds the titles that could not be fetched to the error list.\n",
    "\n",
    "    Parameters:\n",
    "    ------------\n",
    "    titles : str\n",
    "        A string of article titles separated by a delimiter\n",
    "\n",
    "    title_sep : str\n",
    "        The delimiter that separates the article titles in the string\n",
    "\n",
    "    error_list : list\n",
    "        A list to store the titles that could not be fetched\n",
    "\n",
    "    Returns:\n",
    "    ------------\n",
    "    page_info_array : list\n",
    "        A list of dictionaries containing the page information for each article\n",
    "    '''\n",
    "    page_info_array = []\n",
    "    page_info = None\n",
    "\n",
    "    if titles is None:\n",
    "        titles = \"\"\n",
    "    if error_list is None:\n",
    "        error_list = []\n",
    "    try:\n",
    "        json_response = request_pageinfo_per_article(titles)\n",
    "\n",
    "        page_info_dict = json_response['query']['pages']\n",
    "        for page_id, page_info in page_info_dict.items():\n",
    "            try:\n",
    "                page_info_array.append({\n",
    "                    \"page_id\": page_info[\"pageid\"],\n",
    "                    \"title\": page_info[\"title\"],\n",
    "                    \"revision_id\": page_info[\"lastrevid\"],\n",
    "                    \"page_length\": page_info[\"length\"],\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(\"Error while fetching data for: \", page_info[\"title\"])\n",
    "                error_list.append(page_info[\"title\"])\n",
    "                print(e)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Error in API while fetching data\")\n",
    "        error_list.extend(titles.split(title_sep))\n",
    "        print(e)\n",
    "    \n",
    "    return page_info_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list_to_file(file_path, list_to_write):\n",
    "    '''\n",
    "    This function writes a list to a file.\n",
    "\n",
    "    Parameters:\n",
    "    ------------\n",
    "    file_path : str\n",
    "        The path of the file to write the list to\n",
    "    \n",
    "    list_to_write : list\n",
    "        The list to write to the file\n",
    "    '''\n",
    "    with open(file_path, 'w') as file:\n",
    "        for item in list_to_write:\n",
    "            file.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Politicians Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned CSV file that contains the list of politicians by country\n",
    "politicians_by_country_df = pd.read_csv(POLITICIANS_BY_COUNTRY_CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hnaidu36/miniforge3/envs/data512/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while fetching data for:  Barbara Eibinger-Miedl\n",
      "'pageid'\n",
      "Error while fetching data for:  Mehrali Gasimov\n",
      "'pageid'\n",
      "Error while fetching data for:  Kyaw Myint\n",
      "'pageid'\n",
      "Error while fetching data for:  André Ngongang Ouandji\n",
      "'pageid'\n",
      "Error while fetching data for:  Tomás Pimentel\n",
      "'pageid'\n",
      "Error while fetching data for:  Richard Sumah\n",
      "'pageid'\n",
      "Error while fetching data for:  Segun ''Aeroland'' Adewale\n",
      "'pageid'\n",
      "Error while fetching data for:  Bashir Bililiqo\n",
      "'pageid'\n"
     ]
    }
   ],
   "source": [
    "page_info_array = []\n",
    "error_list = []\n",
    "total_len = 0\n",
    "\n",
    "# Get the page info in batches of TITLE_LIMIT\n",
    "# The function np.array_split splits the dataframe into batches of TITLE_LIMIT or TITLE_LIMIT + 1\n",
    "# Hence, we use a TITLE_LIMIT that is less than the actual limit to avoid making a bigger request than the limit\n",
    "for batch in np.array_split(politicians_by_country_df, len(politicians_by_country_df) // TITLE_LIMIT):\n",
    "    titles = batch[\"name\"].tolist()\n",
    "    page_info_array_batch = request_page_info_multiple_articles(TITLE_SEPARATOR.join(titles), TITLE_SEPARATOR, error_list)\n",
    "    page_info_array.extend(page_info_array_batch)\n",
    "\n",
    "page_info_df = pd.DataFrame(page_info_array)\n",
    "\n",
    "# Write the error list to a file\n",
    "write_list_to_file(ERROR_LIST_PATH, error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rate: 0.11262846684499507%\n"
     ]
    }
   ],
   "source": [
    "# Calculate Error Rate\n",
    "error_rate = len(error_list) / page_info_df['page_id'].nunique()\n",
    "print(f\"Error Rate: {error_rate * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, there are duplicates in the politicians_by_country_df dataframe. We'll drop the duplicates in the page_info_df dataframe, and keep the first occurrence. \n",
    "\n",
    "(Ideally, we should have dropped the duplicates in the politicians_by_country_df dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7103, 4)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_info_df = page_info_df.drop_duplicates(subset=['page_id'])\n",
    "page_info_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the final page_info dataframe to a CSV file\n",
    "page_info_df.to_csv(WP_POLITICIANS_WITH_PAGE_INFO_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Article Quality Predictions\n",
    "\n",
    "Once the latest revision ids are extracted for each article, using the Page Info API, the ORES API can be used to get the Quality Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    IMPORTS\n",
    "#\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "# The path to the .env file that contains the Wikimedia credentials\n",
    "ENV_PATH = \".env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(ENV_PATH)\n",
    "\n",
    "USERNAME = os.getenv(\"WIKIMEDIA_USERNAME\")\n",
    "ACCESS_TOKEN = os.getenv(\"WIKIMEDIA_ACCESS_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflections and Implications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data512",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
